{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da62758b",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a061f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os, sys\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Layer\n",
    "\n",
    "sys.path.insert(1, str(os.path.abspath(os.path.join(os.getcwd(), os.pardir))) + \"\\\\Timestamp2Vec\\\\\")\n",
    "from helper_functions import *\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f786499",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = os.path.join(os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop'), 'data_thesis')\n",
    "data = np.load(data_location + \"/vectorized_dates.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the Booleans to int\n",
    "data = np.asarray(data, dtype=\"float64\")\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767005b",
   "metadata": {},
   "source": [
    "### Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d88cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test split, using SEED\n",
    "train_data, test_data, _, _ = train_test_split(\n",
    "    data, data, test_size=0.2, random_state=SEED\n",
    ")\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f25331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up space\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data to [0, 1]\n",
    "# obtain the min and max value\n",
    "min_val = np.amin(train_data, axis=0)\n",
    "max_val = np.amax(train_data, axis=0)\n",
    "\n",
    "# take half of the training set for \"testing\"\n",
    "\n",
    "train_data = train_data[:round(len(train_data) / 5)]\n",
    "test_data = test_data[:round(len(test_data) / 5)]\n",
    "\n",
    "\n",
    "# normalize the train and test data\n",
    "train_data = normalize(train_data)\n",
    "test_data = normalize(test_data)\n",
    "\n",
    "# store tensors on CPU to save enough space on GPU\n",
    "with tf. device(\"cpu:0\"):\n",
    "    train_data = tf.cast(train_data, tf.float32)\n",
    "    test_data = tf.cast(test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499657bd",
   "metadata": {},
   "source": [
    "# Variational Autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb067d",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00902a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENT_DIM = 6\n",
    "X_SHAPE = train_data[2].shape[0]\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b32c8",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e3427",
   "metadata": {},
   "source": [
    "### Sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9798bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    # Sampling layer of the VAE, creation of the latent variable z\n",
    "    # The sampling layer uses as distribution a normal distribution\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # obtain the mean and the logvar's of each dimension\n",
    "        z_mean, z_log_var = inputs\n",
    "        # get the batchsize\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        # get the dimension of the data\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        # sample random values from the normal distribution\n",
    "        epsilon = tf.keras.backend.random_normal(shape =(batch, dim))\n",
    "        # perform the sample step\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d30b9f",
   "metadata": {},
   "source": [
    "#### VAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc957ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(Model):\n",
    "  def __init__(self, encoder, decoder, X_SHAPE):\n",
    "    super(VariationalAutoEncoder, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.X_shape = X_SHAPE\n",
    "\n",
    "    \n",
    "  def train_step(self, x):\n",
    "    if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "      # map to latent space and obtain z_mean, z_log_var, z\n",
    "      z_mean, z_log_var, z = self.encoder(x)\n",
    "      # decode z to obtain the reconstruction\n",
    "      decoded = self.decoder(z)\n",
    "      # obtain the reconstruction loss\n",
    "      reconstruction_loss = tf.reduce_mean(\n",
    "              keras.losses.mean_squared_error(x, decoded)\n",
    "            )\n",
    "      reconstruction_loss *= X_SHAPE\n",
    "      # obtain the kl_loss\n",
    "      kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "      kl_loss = tf.reduce_mean(kl_loss)\n",
    "      total_loss = reconstruction_loss + kl_loss\n",
    "    grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "    return {\n",
    "      \"loss\": total_loss,\n",
    "      \"reconstruction_loss\": reconstruction_loss,\n",
    "      \"kl_loss\": kl_loss,\n",
    "      }\n",
    "\n",
    "\n",
    "  def test_step(self, x):\n",
    "    if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "    # map to latent space and obtain z_mean, z_log_var, z\n",
    "    z_mean, z_log_var, z = self.encoder(x)\n",
    "    # decode z to obtain the reconstruction\n",
    "    decoded = self.decoder(z)\n",
    "    # obtain the reconstruction loss\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "            keras.losses.mean_squared_error(x, decoded)\n",
    "          )\n",
    "    reconstruction_loss *= X_SHAPE\n",
    "    # obtain the kl_loss\n",
    "    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    return {\n",
    "      \"loss\": total_loss,\n",
    "      \"reconstruction_loss\": reconstruction_loss,\n",
    "      \"kl_loss\": kl_loss,\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31a2f8",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622efb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(variational_autoencoder):\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode = 'min', restore_best_weights=True)\n",
    "        history = variational_autoencoder.fit(train_data, train_data, \n",
    "                epochs=EPOCHS, \n",
    "                batch_size=BATCH_SIZE,\n",
    "                validation_data=(test_data, test_data),\n",
    "                shuffle=True,\n",
    "                verbose=0,\n",
    "                        callbacks=[callback])\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6b6ea",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6356bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(model_params):\n",
    "    # define the input of the encoder\n",
    "    input_encoder = keras.Input(shape = (X_SHAPE,))\n",
    "\n",
    "    # define the layers of the model\n",
    "    x = Dense(first_layer[model_params[0]], activation=activations[model_params[3]])(input_encoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(second_layer[model_params[1]], activation=activations[model_params[3]])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(third_layer[model_params[2]], activation=activations[model_params[3]])(x)\n",
    "    \n",
    "    # obtain the mean and variance to sample from\n",
    "    z_mean = Dense(latent_dimensions[model_params[4]], name =\"z_mean\")(x)\n",
    "    z_log_var = Dense(latent_dimensions[model_params[4]], name =\"z_log_var\")(x)\n",
    "    # sample, using the z_mean and z_log_var\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    # define the encoder model\n",
    "    encoder = Model(input_encoder, [z_mean, z_log_var, z], name =\"encoder\")\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(model_params):\n",
    "    input_decoder = keras.Input(shape = latent_dimensions[model_params[4]],)\n",
    "    # define the layers of the model\n",
    "    x = Dense(third_layer[model_params[2]], activation=activations[model_params[3]])(input_decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(second_layer[model_params[1]], activation=activations[model_params[3]])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(first_layer[model_params[0]], activation=activations[model_params[3]])(x)\n",
    "    decoder_output = Dense(X_SHAPE, activation=\"sigmoid\")(x)\n",
    "\n",
    "    decoder = Model(input_decoder, decoder_output, name =\"decoder\")\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vae(model_params):\n",
    "    encoder = create_encoder(model_params)\n",
    "    decoder = create_decoder(model_params)\n",
    "    vae = VariationalAutoEncoder(encoder, decoder, X_SHAPE)\n",
    "    vae.compile(optimizer=\"adam\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c2021",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc5dc8",
   "metadata": {},
   "source": [
    "## EA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA(object):\n",
    "    def __init__(self, pop_size, first_layer, second_layer, third_layer, activations, latent_dimensions, a, penalty, max_number_params, vae_architecture):\n",
    "        self.pop_size = pop_size\n",
    "        self.first_layer  = first_layer\n",
    "        self.second_layer = second_layer\n",
    "        self.third_layer = third_layer\n",
    "        self.activations = activations\n",
    "        self.latent_dimensions = latent_dimensions\n",
    "        self.a = a\n",
    "        self.penalty = penalty\n",
    "        self.max_number_params = max_number_params\n",
    "        self.vae_architecture = vae_architecture\n",
    "        \n",
    "    \n",
    "    def evaluate(self, x):\n",
    "        # obtain the model and optimizer of the individual\n",
    "        model = self.vae_architecture(x)\n",
    "        # obtain the history of training\n",
    "        history = train(model)\n",
    "        # obtain most recent val_loss\n",
    "        val_loss = history.history[\"val_loss\"][-1]\n",
    "        # calculate the number of parameters of the model\n",
    "        number_param = model.encoder.count_params() + model.decoder.count_params()\n",
    "        # evaluate the model\n",
    "        evaluation = val_loss + self.penalty * (number_param / self.max_number_params)\n",
    "        return evaluation\n",
    "    \n",
    "    \n",
    "    def check_mutation(self, x, option_list):\n",
    "        if np.around(x) in range(len(option_list)):\n",
    "            # if the mutated element is an option of the option_list\n",
    "            return int(np.around(x))\n",
    "        else:\n",
    "            # if the mutated element is not an option of the option_list, return a random element\n",
    "            return np.random.randint(0, len(option_list))\n",
    "    \n",
    "    \n",
    "    def mutate(self, x1, x2, x3):\n",
    "        # create a mutation based on the 3 individuals of the triple \n",
    "        mutated = x1 + (self.a * (x3 - x2))\n",
    "        # check for each element of the mutation is valid (whether the index is a valid index)\n",
    "        for i in range(mutated.shape[0]):\n",
    "            if i < mutated.shape[0] - 2:\n",
    "                mutated[i] = self.check_mutation(mutated[i], self.first_layer)\n",
    "            elif i == mutated.shape[0] - 1:\n",
    "                # activation\n",
    "                mutated[i] = (self.check_mutation(mutated[i], self.activations))\n",
    "            else:\n",
    "                # latent dim\n",
    "                mutated[i] = (self.check_mutation(mutated[i], self.latent_dimensions))\n",
    "        return mutated\n",
    "    \n",
    "    \n",
    "    def recombine(self, candidate, mutation):\n",
    "        # for each element of the candidate\n",
    "        for i in range(candidate.shape[0]):\n",
    "            # sample a random integer between 0 and 2 (probability of an element being recombined or not)\n",
    "            # (0, 2) is used, because (0, 1) only returns 0\n",
    "            prob = np.random.randint(0, 2)\n",
    "            # if the sampled integer is equal to 1, recombine. Else do nothing\n",
    "            if prob == 1:\n",
    "                candidate[i] = mutation[i]\n",
    "        return candidate\n",
    "    \n",
    "    \n",
    "    def select_triple(self, ind_x, population):\n",
    "        # select 3 random individuals of the population (3 random integers)\n",
    "        x1, x2, x3 = np.random.choice(range(len(population))), np.random.choice(range(len(population))), np.random.choice(range(len(population)))\n",
    "        # check whether there are 2 identical individuals (indices) (and the inviduals are not the same as the candidate)\n",
    "        while ind_x in [x1, x2, x3] or x1 in [x2, x3] or x2 == x3:\n",
    "            # if the same --> select 3 random individuals of the population\n",
    "            x1, x2, x3 = np.random.choice(range(len(population))), np.random.choice(range(len(population))), np.random.choice(range(len(population)))\n",
    "        # return the 3 individuals (the individuals with the corresponding indices in the population)\n",
    "        return population[x1], population[x2], population[x3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, x_old, f_old):\n",
    "        x_new, f_new = np.copy(x_old), np.copy(f_old)\n",
    "        for i in range(self.pop_size):\n",
    "            # for each individual of the population:\n",
    "            # make a copy of the individual (if the candidate is worse than the original individual,\n",
    "            # we do not want the individual to be changed already)\n",
    "            candidate = np.copy(x_new[i])\n",
    "            # select 3 random individuals of the population, that are not equal to each other and\n",
    "            # are not equal to the candidate\n",
    "            x1, x2, x3 = self.select_triple(i, x_new)\n",
    "            # mutate the triple\n",
    "            mutated_triple = self.mutate(x1, x2, x3)\n",
    "            # recombine the mutated triple with the candidate\n",
    "            candidate = self.recombine(candidate, mutated_triple)\n",
    "            # train and evaluate the candidate\n",
    "            f_candidate = self.evaluate(candidate)\n",
    "            if f_candidate < f_new[i]:\n",
    "                # if the candidate is better than the original version, replace it in the population\n",
    "                x_new[i] = candidate\n",
    "                f_new[i] = f_candidate\n",
    "        return x_new, f_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da858e5a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_population(pop_size, first_layer, second_layer, third_layer, activations, latent_dimensions):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        individual = []\n",
    "        individual.append(np.random.randint(0, len(first_layer)))\n",
    "        individual.append(np.random.randint(0, len(second_layer)))\n",
    "        individual.append(np.random.randint(0, len(third_layer)))\n",
    "        individual.append(np.random.randint(0, len(activations)))\n",
    "        individual.append(np.random.randint(0, len(latent_dimensions)))\n",
    "        model_params = np.asarray(individual, dtype=object)\n",
    "        population.append(model_params)\n",
    "    return np.asarray(population)\n",
    "\n",
    "def evaluate_init_pop(ea, population):\n",
    "    result = []\n",
    "    for i in range(population.shape[0]):\n",
    "        print(\"Individual %d\" %i)\n",
    "        individual = population[i]\n",
    "        evaluation = ea.evaluate(individual)\n",
    "        result.append(evaluation)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a76ed",
   "metadata": {},
   "source": [
    "## Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73564abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 10 # if necessary, please increase the number of generations\n",
    "pop_size = 10\n",
    "a = 1.0\n",
    "penalty = 0.01\n",
    "max_number_params = 1134\n",
    "\n",
    "# lists with different options\n",
    "first_layer = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "second_layer = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "third_layer = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "activations = [\"relu\", \"sigmoid\", \"tanh\", \"elu\"]\n",
    "latent_dimensions = [2, 4, 6, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89356fe3",
   "metadata": {},
   "source": [
    "## Run EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e24c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ea = EA(pop_size, first_layer, second_layer, third_layer, activations, latent_dimensions, a, penalty, max_number_params, create_vae)\n",
    "# init\n",
    "pop = init_population(pop_size, first_layer, second_layer, third_layer, activations, latent_dimensions)\n",
    "f = evaluate_init_pop(ea, pop)\n",
    "\n",
    "# We want to gather populations and values of the best candidates to further\n",
    "# analyze the algorithm.\n",
    "populations = []\n",
    "populations.append(pop)\n",
    "f_best = [f.min()]\n",
    "\n",
    "# Run the EA.\n",
    "for i in range(num_generations):\n",
    "    print('Generation: {}, best fitness: {:.5f}'.format(i, f.min()))\n",
    "    current_best = np.where(f == f.min())[0][0]\n",
    "    np.save(\"Data/important_variables/population.npy\", populations[len(populations) - 1])\n",
    "    np.save(\"Data/important_variables/best.npy\", pop[current_best])\n",
    "    pop, f = ea.step(pop, f)\n",
    "    populations.append(pop)\n",
    "    if f.min() < f_best[-1]:\n",
    "        f_best.append(f.min())\n",
    "    else:\n",
    "        f_best.append(f_best[-1])\n",
    "print('FINISHED!')\n",
    "# obtain the best model of the population of models (with the best fitness)\n",
    "index_best_parameters = np.where(f == f.min())[0][0]\n",
    "best_model = create_vae(pop[index_best_parameters])\n",
    "print(\"Best model, encoder: \")\n",
    "print(best_model.encoder.summary())\n",
    "print(\"Best model, decoder: \")\n",
    "print(best_model.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_parameters = np.where(f == f.min())[0][0]\n",
    "best_model = create_vae(pop[index_best_parameters])\n",
    "print(\"Best model, encoder: \")\n",
    "print(best_model.encoder.summary())\n",
    "print(\"Best model, decoder: \")\n",
    "print(best_model.decoder.summary())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5f809e317b75bf44c1fc96d1cd9e1131b9cde9a3e6db8a0d8be4378aec00bbc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
